TODO
- AutoGrad with DAG. Each node is a tensor and edge a operation. Tracks everything and backprop just goes back
- Layers Linear, with forward __call__ function
- Cost Function
- More Layers like Conv2D, LSTM, RNN, Sequential, GRU, Embedding
- custom cuda kernels for autograd

FUTURE GOALS
- Reinforcement learning parallel training/hybrid training 
- adaptive exploration trat
- custom CUDA kernels for rl
- transfer learning
- environment wrappers
- visualization tools 
- customaziable rl building blocks
