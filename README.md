TODO
- AutoGrad with DAG. Each node is a tensor and edge a operation. Tracks every operation with requires_grad and backprop just performs chain rule
- Layers Linear, with forward __call__ function (some what started)
- Cost Function
- More Layers like Conv2D, LSTM, RNN, Sequential, GRU, Embedding, Graph neural nets, Liquid Neural Nets, HSTU
- custom cuda kernels for

FUTURE GOALS
- Reinforcement learning parallel training/hybrid training 
- adaptive exploration trat
- custom CUDA kernels for rl
- transfer learning
- environment wrappers
- visualization tools 
- customaziable rl building blocks
